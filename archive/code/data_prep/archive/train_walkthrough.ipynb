{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ambient and attack directories.\n",
      "Loading CAN metadata...\n",
      "Loading raw can data...\n",
      "Saving parquet files...\n",
      "Processing raw can data...\n",
      "Done processing CAN data.\n",
      "Saving processed parquet files...\n",
      "Done saving processed parquet files.\n",
      "Loading processing data into 'CanData' structure\n"
     ]
    }
   ],
   "source": [
    "from CANDataset import CANDataset\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "dataset = CANDataset(data_path, log_verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANnoloAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings, feature_vec_length):\n",
    "        super(CANnoloAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.encoder_dense = nn.Linear(embedding_dim+feature_vec_length, dense_units)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_lstm = nn.LSTM(input_size=dense_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(input_size=lstm_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(lstm_units, feature_vec_length)\n",
    "        self.decoder_output = nn.Sigmoid()  # To reconstruct the original packets\n",
    "\n",
    "    def forward(self, can_ids, features):\n",
    "        # Encoding\n",
    "        embedded_ids = self.embedding(can_ids)\n",
    "        # You might need to concatenate the embedded IDs with other features\n",
    "        x = torch.cat([embedded_ids, features], dim=1)\n",
    "        x = torch.tanh(self.encoder_dense(x))\n",
    "        x = self.encoder_dropout(x)\n",
    "        x, _ = self.encoder_lstm(x)\n",
    "\n",
    "        # Decoding\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        x = self.decoder_dense(x)\n",
    "        reconstructed = self.decoder_output(x)\n",
    "\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>aid</th>\n",
       "      <th>data</th>\n",
       "      <th>delta_time_last_msg</th>\n",
       "      <th>delta_time_last_same_aid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>852</td>\n",
       "      <td>1FFF40000003C580</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>1505</td>\n",
       "      <td>893FE0070A000080</td>\n",
       "      <td>1.192093e-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>651</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000992</td>\n",
       "      <td>167</td>\n",
       "      <td>005108E5112A00A0</td>\n",
       "      <td>9.899139e-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000994</td>\n",
       "      <td>722</td>\n",
       "      <td>0000500000000000</td>\n",
       "      <td>2.145767e-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204753</th>\n",
       "      <td>86.459022</td>\n",
       "      <td>560</td>\n",
       "      <td>F700000A7C000E00</td>\n",
       "      <td>9.698868e-04</td>\n",
       "      <td>0.019958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204754</th>\n",
       "      <td>86.461950</td>\n",
       "      <td>339</td>\n",
       "      <td>00000000000C1002</td>\n",
       "      <td>2.928019e-03</td>\n",
       "      <td>0.019914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204755</th>\n",
       "      <td>86.462905</td>\n",
       "      <td>1634</td>\n",
       "      <td>4E60000040000000</td>\n",
       "      <td>9.551048e-04</td>\n",
       "      <td>0.019882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204756</th>\n",
       "      <td>86.462906</td>\n",
       "      <td>412</td>\n",
       "      <td>02FC200002002730</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.019881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204757</th>\n",
       "      <td>86.462907</td>\n",
       "      <td>14</td>\n",
       "      <td>2054560208087530</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.009924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192017 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time   aid              data  delta_time_last_msg  \\\n",
       "0        0.000000   852  1FFF40000003C580         0.000000e+00   \n",
       "1        0.000001  1505  893FE0070A000080         1.192093e-06   \n",
       "2        0.000002   651  0000000000000000         9.536743e-07   \n",
       "3        0.000992   167  005108E5112A00A0         9.899139e-04   \n",
       "4        0.000994   722  0000500000000000         2.145767e-06   \n",
       "...           ...   ...               ...                  ...   \n",
       "204753  86.459022   560  F700000A7C000E00         9.698868e-04   \n",
       "204754  86.461950   339  00000000000C1002         2.928019e-03   \n",
       "204755  86.462905  1634  4E60000040000000         9.551048e-04   \n",
       "204756  86.462906   412  02FC200002002730         9.536743e-07   \n",
       "204757  86.462907    14  2054560208087530         9.536743e-07   \n",
       "\n",
       "        delta_time_last_same_aid  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "204753                  0.019958  \n",
       "204754                  0.019914  \n",
       "204755                  0.019882  \n",
       "204756                  0.019881  \n",
       "204757                  0.009924  \n",
       "\n",
       "[192017 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.attack_data.accelerator_attack_drive_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define config\n",
    "This is what we feed to the CANDataset object to create a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"delta_time_last_msg\": {\n",
    "        \"specific_to_can_id\": False,\n",
    "        \"records_back\": 30\n",
    "    },\n",
    "    \"delta_time_last_same_aid\": {\n",
    "        \"specific_to_can_id\": True,\n",
    "        \"records_back\": 15\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `get_dataloaders` on CANDataset object to get the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_loader, validation_loader, attack_loader = dataset.get_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "From the config we defined:\n",
    "    - Batch size of `32`\n",
    "    - Keep track of the current Can ID.\n",
    "    - want the last `30` `delta_time_last_msg`\n",
    "    - want the last `15` `delta_time_last_same_aid`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1760, 1176,  560,  778,  452,  676,  813, 1277,  339,   14, 1634,  737,\n",
       "          852, 1505,  412,  208,   51,  628,  192,  293,  354,  167, 1760, 1175,\n",
       "           60,  519,  263, 1225,  470,  208,   14,   51]),\n",
       " tensor([[0.0000e+00, 1.9073e-06, 9.5367e-07,  ..., 9.0060e-03, 1.0972e-02,\n",
       "          1.0019e-02],\n",
       "         [1.9073e-06, 9.5367e-07, 1.1921e-06,  ..., 2.0578e-02, 1.8947e-02,\n",
       "          1.9974e-02],\n",
       "         [9.5367e-07, 1.1921e-06, 9.9492e-04,  ..., 1.9925e-02, 2.0006e-02,\n",
       "          1.9995e-02],\n",
       "         ...,\n",
       "         [1.9073e-06, 9.5367e-07, 9.5367e-07,  ..., 7.9238e-03, 1.2084e-02,\n",
       "          7.9310e-03],\n",
       "         [9.5367e-07, 9.5367e-07, 1.0362e-03,  ..., 1.0004e-02, 9.9571e-03,\n",
       "          1.0008e-02],\n",
       "         [9.5367e-07, 1.0362e-03, 9.5367e-07,  ..., 7.9730e-03, 1.3045e-02,\n",
       "          8.0020e-03]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_data = ambient_loader.__getitem__(0) # input normally acts as index, but this does not really work as an index. More like get next item.\n",
    "display(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Represents Can ID: \n",
      "204\n",
      "\n",
      "Represents Feature Vector: \n",
      "tensor([0.0000e+00, 9.5367e-07, 9.0849e-03, 9.9890e-03, 2.0004e-02, 9.7489e-04,\n",
      "        1.9014e-02, 9.9111e-04, 6.4800e-03, 1.0200e-03, 9.5367e-07, 9.5367e-07,\n",
      "        1.2046e-02, 1.0159e-03, 1.8436e-02, 1.0025e-02, 1.0021e-02, 9.9802e-04,\n",
      "        1.0180e-03, 1.7963e-02, 1.7386e-02, 2.6519e-03, 1.0040e-03, 1.9073e-06,\n",
      "        1.1921e-06, 1.0140e-03, 1.9073e-06, 9.5367e-07, 1.0171e-03, 9.5367e-07,\n",
      "        1.0000e+00, 1.0000e+00, 1.0001e+00, 9.9998e-01, 1.0000e+00, 9.9993e-01,\n",
      "        1.0001e+00, 1.0001e+00, 9.9992e-01, 1.0000e+00, 9.9998e-01, 1.0021e+00,\n",
      "        9.9996e-01, 9.9994e-01, 1.0002e+00])\n"
     ]
    }
   ],
   "source": [
    "test_batch_can_ids, test_feature_vec = example_data\n",
    "\n",
    "print(f'Represents Can ID: \\n{test_batch_can_ids[0]}\\n')\n",
    "print(f'Represents Feature Vector: \\n{test_feature_vec[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `example_data` is a tuple containing a list of 32 (batch_size) Can ID's and the feature vectors defined in the config.\n",
    "\n",
    "([`tensor containing Can ID's`],[`tensor containing features`])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CAN IDs: 105\n",
      "Feature vector length: 45\n"
     ]
    }
   ],
   "source": [
    "unique_can_ids = dataset.get_unique_can_ids()\n",
    "num_can_ids = len(unique_can_ids)\n",
    "feature_vec_length = ambient_loader.features_len\n",
    "print(f\"Number of CAN IDs: {num_can_ids}\")\n",
    "print(f\"Feature vector length: {feature_vec_length-1}\") # minus one because the can id is the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 1789\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "lstm_units = 128 # defined in canolo paper\n",
    "dense_units = 256 # defined in canolo paper\n",
    "dropout_rate = 0.2 # defined in canolo paper\n",
    "num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "print(f\"Number of embeddings: {num_embeddings}\")\n",
    "\n",
    "# Model\n",
    "model = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = ambient_loader.batch_size\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.544073104858398\n",
      "Time per batch: 0.06544073104858399\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    ambient_loader.__getitem__(0)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Running a forward pass with a batch of data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reconstructed_output \u001b[39m=\u001b[39m model(test_batch_can_ids, test_feature_vec)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mse_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m error \u001b[39m=\u001b[39m mse_loss(reconstructed_output, test_feature_vec)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, can_ids, features):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     embedded_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(can_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# You might need to concatenate the embedded IDs with other features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([embedded_ids, features], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Running a forward pass with a batch of data\n",
    "reconstructed_output = model(test_batch_can_ids, test_feature_vec)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "error = mse_loss(reconstructed_output, test_feature_vec)\n",
    "print(\"Reconstruction Error:\", error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_sec = 0.08151132106781006 * ambient_loader.num_batches\n",
    "print(f\"Time in seconds: {time_in_sec}\")\n",
    "time_in_min = time_in_sec / 60\n",
    "print(f\"Time in minutes: {time_in_min}\")\n",
    "time_in_hours = time_in_min / 60\n",
    "print(f\"Time in hours: {time_in_hours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 23\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# time the training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m train_model(model, ambient_loader, validation_loader, loss_fn, optimizer, num_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime taken: \u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m-\u001b[39mstart\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m num_processed_batches_in_epoch \u001b[39m=\u001b[39m train_loader\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m PSEUDO_EPOCH_SIZE\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     can_ids, features \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# can_ids, features = can_ids.to('mps'), features.to('mps')\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CANIDS/code/data_prep/TorchLoader.py:34\u001b[0m, in \u001b[0;36mCANDataLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# can_data = [df1,df2,df3]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m current_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_df_index]\n\u001b[0;32m---> 34\u001b[0m processed_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(current_df)\n\u001b[1;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(processed_batch) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_df_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/CANIDS/code/data_prep/TorchLoader.py:69\u001b[0m, in \u001b[0;36mCANDataLoader.extract_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m feature_name, feature_config \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m feature_config[\u001b[39m'\u001b[39m\u001b[39mspecific_to_can_id\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m         history \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39maid\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m row[\u001b[39m'\u001b[39;49m\u001b[39maid\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39miloc[:index]\n\u001b[1;32m     70\u001b[0m         history \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mtail(feature_config[\u001b[39m'\u001b[39m\u001b[39mrecords_back\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     71\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/frame.py:3887\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3885\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3886\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3887\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[1;32m   3889\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3890\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3891\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/frame.py:3949\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3946\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   3948\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 3949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/generic.py:4088\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m, indices, axis: Axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   4079\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4081\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4086\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4087\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4088\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   4089\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4090\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/generic.py:4068\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[39m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4064\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\n\u001b[1;32m   4065\u001b[0m         indices\u001b[39m.\u001b[39mstart, indices\u001b[39m.\u001b[39mstop, indices\u001b[39m.\u001b[39mstep, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp\n\u001b[1;32m   4066\u001b[0m     )\n\u001b[0;32m-> 4068\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[1;32m   4069\u001b[0m     indices,\n\u001b[1;32m   4070\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[1;32m   4071\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   4072\u001b[0m )\n\u001b[1;32m   4073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[39m=\u001b[39mnew_data\u001b[39m.\u001b[39maxes)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   4074\u001b[0m     \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4075\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/internals/managers.py:877\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    874\u001b[0m indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[1;32m    876\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 877\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m    878\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[1;32m    879\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[1;32m    880\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    881\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    882\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    883\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/internals/managers.py:670\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    663\u001b[0m     new_blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    664\u001b[0m         indexer,\n\u001b[1;32m    665\u001b[0m         fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m    666\u001b[0m         only_slice\u001b[39m=\u001b[39monly_slice,\n\u001b[1;32m    667\u001b[0m         use_na_proxy\u001b[39m=\u001b[39muse_na_proxy,\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m    671\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    672\u001b[0m             indexer,\n\u001b[1;32m    673\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    674\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    675\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    676\u001b[0m             ),\n\u001b[1;32m    677\u001b[0m         )\n\u001b[1;32m    678\u001b[0m         \u001b[39mfor\u001b[39;49;00m blk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks\n\u001b[1;32m    679\u001b[0m     ]\n\u001b[1;32m    681\u001b[0m new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m    682\u001b[0m new_axes[axis] \u001b[39m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/internals/managers.py:671\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    663\u001b[0m     new_blocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    664\u001b[0m         indexer,\n\u001b[1;32m    665\u001b[0m         fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m    666\u001b[0m         only_slice\u001b[39m=\u001b[39monly_slice,\n\u001b[1;32m    667\u001b[0m         use_na_proxy\u001b[39m=\u001b[39muse_na_proxy,\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 671\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    672\u001b[0m             indexer,\n\u001b[1;32m    673\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    674\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    675\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    676\u001b[0m             ),\n\u001b[1;32m    677\u001b[0m         )\n\u001b[1;32m    678\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    679\u001b[0m     ]\n\u001b[1;32m    681\u001b[0m new_axes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n\u001b[1;32m    682\u001b[0m new_axes[axis] \u001b[39m=\u001b[39m new_axis\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1061\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1061\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m   1062\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[1;32m   1063\u001b[0m )\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[39m#  these assertions\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1068\u001b[0m     \u001b[39m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     \u001b[39m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/array_algos/take.py:118\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[1;32m    117\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/pandas/core/array_algos/take.py:163\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m flip_order:\n\u001b[1;32m    166\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PSEUDO_EPOCH_SIZE = 3000\n",
    "\n",
    "def validate_model(model, validation_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches_to_validate = 1000\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for i, batch in enumerate(validation_loader):\n",
    "            can_ids, features = batch\n",
    "            # can_ids, features = can_ids.to('mps'), features.to('mps')\n",
    "            \n",
    "            if i == num_batches_to_validate:\n",
    "                break\n",
    "            \n",
    "            # Forward pass: compute the model output\n",
    "            reconstructed = model(can_ids, features)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    model.train()  # Revert to training mode\n",
    "    num_processed_batches = validation_loader.batch_size * num_batches_to_validate\n",
    "    avg_loss = total_loss / num_processed_batches\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, loss_fn, optimizer, num_epochs):\n",
    "    total_train_loss = 0\n",
    "    pseudo_epoch = 1\n",
    "    num_processed_batches_in_epoch = train_loader.batch_size * PSEUDO_EPOCH_SIZE\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        can_ids, features = batch\n",
    "        # can_ids, features = can_ids.to('mps'), features.to('mps')\n",
    "        print(f\"{i}\", end=\"\\r\")\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        reconstructed = model(can_ids, features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if i % PSEUDO_EPOCH_SIZE == 0:\n",
    "\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Validate model\n",
    "            validation_loss = validate_model(model, validation_loader, loss_fn)\n",
    "            print(f\"Psuedo Epoch {pseudo_epoch}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "            # Show training progress\n",
    "            avg_train_loss = total_train_loss / num_processed_batches_in_epoch\n",
    "            print(f\"Epoch {pseudo_epoch-1}, Average Training Loss: {avg_train_loss}\")\n",
    "            \n",
    "            if pseudo_epoch > num_epochs:\n",
    "                break\n",
    "            \n",
    "\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), f'./saved_model/canolo_model_{pseudo_epoch}.pt')\n",
    "\n",
    "            # save metadata\n",
    "            total_batches_processed = i\n",
    "\n",
    "            metadata = {\n",
    "                \"total_batches_processed\": total_batches_processed,\n",
    "                \"total_train_loss\": total_train_loss,\n",
    "                \"avg_train_loss\": avg_train_loss,\n",
    "                \"validation_loss\": validation_loss\n",
    "            }\n",
    "\n",
    "            with open(f'training_metadata.tsv', 'a') as f:\n",
    "                f.write('\\t'.join(str(metadata[key]) for key in metadata.keys()) + '\\n')\n",
    "\n",
    "            pseudo_epoch += 1\n",
    "            total_train_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "train_model(model, ambient_loader, validation_loader, loss_fn, optimizer, num_epochs)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2092159968.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Psuedo Epoch 1, Validation Loss: 0.0014687743630202022\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Psuedo Epoch 1, Validation Loss: 0.0014687743630202022\n",
    "Epoch 0, Average Training Loss: 0.002530826270366864\n",
    "Psuedo Epoch 2, Validation Loss: 0.001387331632229234\n",
    "Epoch 1, Average Training Loss: 0.0014076253618695773\n",
    "Time taken: 964.5090320110321\n",
    "Time per batch: 9.64509032011032\n",
    "\n",
    "without cuda:\n",
    "\n",
    "Psuedo Epoch 1, Validation Loss: 0.0020492164343595505\n",
    "Epoch 0, Average Training Loss: 0.002164360677629399\n",
    "Psuedo Epoch 2, Validation Loss: 0.0020707388259470464\n",
    "Epoch 1, Average Training Loss: 0.002073641937187252\n",
    "Time taken: 985.8024809360504\n",
    "Time per batch: 9.858024809360504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341.78125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in enumerate(ambient_loader.can_data):\n",
    "    len(ambient_loader.can_data[i])/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.8549"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(ambient_loader)/3000) * 6) /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CANnoloAutoencoder(\n",
       "  (embedding): Embedding(1789, 105)\n",
       "  (encoder_dense): Linear(in_features=150, out_features=256, bias=True)\n",
       "  (encoder_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (encoder_lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_dense): Linear(in_features=128, out_features=45, bias=True)\n",
       "  (decoder_output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 1: Initialize the model\n",
    "# # Hyperparameters\n",
    "# embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "# lstm_units = 128 # defined in canolo paper\n",
    "# dense_units = 256 # defined in canolo paper\n",
    "# dropout_rate = 0.2 # defined in canolo paper\n",
    "# num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# # Model\n",
    "# model2 = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# # Step 2: Load the state dictionary\n",
    "# state_dict = torch.load(\"./saved_model/canolo_model_1.pt\")\n",
    "# model2.load_state_dict(state_dict)\n",
    "\n",
    "# # If you want to use the model for inference, switch to evaluation mode\n",
    "# model2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
