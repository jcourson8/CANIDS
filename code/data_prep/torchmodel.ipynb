{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanDataset import CanDataset\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "dataset = CanDataset(data_path, log_verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANnoloAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings):\n",
    "        super(CANnoloAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.encoder_dense = nn.Linear(embedding_dim+45, dense_units)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_lstm = nn.LSTM(input_size=dense_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(input_size=lstm_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(lstm_units, 45)\n",
    "        self.decoder_output = nn.Sigmoid()  # To reconstruct the original packets\n",
    "\n",
    "    def forward(self, can_ids, features):\n",
    "        # Encoding\n",
    "        embedded_ids = self.embedding(can_ids)\n",
    "        # You might need to concatenate the embedded IDs with other features\n",
    "        x = torch.cat([embedded_ids, features], dim=1)\n",
    "        x = torch.tanh(self.encoder_dense(x))\n",
    "        x = self.encoder_dropout(x)\n",
    "        x, _ = self.encoder_lstm(x)\n",
    "\n",
    "        # Decoding\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        x = self.decoder_dense(x)\n",
    "        reconstructed = self.decoder_output(x)\n",
    "\n",
    "        return reconstructed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANID   -\n",
    "# f1      -        -  reconstructed_f1\n",
    "# f2      -   -    -  reconstructed_f2\n",
    "# f3      -        -  reconstructed_f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.attack_data.accelerator_attack_drive_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define config\n",
    "This is what we feed to the CanDataset object to create a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"delta_time_last_msg\": {\n",
    "        \"specific_to_can_id\": False,\n",
    "        \"records_back\": 30\n",
    "    },\n",
    "    \"delta_time_last_same_aid\": {\n",
    "        \"specific_to_can_id\": True,\n",
    "        \"records_back\": 15\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `get_dataloaders` on CanDataset object to get the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_loader, validation_loader, attack_loader = dataset.get_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "From the config we defined:\n",
    "    - Batch size of `32`\n",
    "    - Keep track of the current Can ID.\n",
    "    - want the last `30` `delta_time_last_msg`\n",
    "    - want the last `15` `delta_time_last_same_aid`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = ambient_loader.__getitem__(0) # input normally acts as index, but this does not really work as an index. More like get next item.\n",
    "display(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_can_ids, test_feature_vec = example_data\n",
    "\n",
    "print(f'Represents Can ID: \\n{test_batch_can_ids[0]}\\n')\n",
    "print(f'Represents Feature Vector: \\n{test_feature_vec[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `example_data` is a tuple containing a list of 32 (batch_size) Can ID's and the feature vectors defined in the config.\n",
    "\n",
    "([`tensor containing Can ID's`],[`tensor containing features`])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_can_ids = dataset.get_unique_can_ids()\n",
    "num_can_ids = len(unique_can_ids)\n",
    "feature_vec_length = ambient_loader.features_len\n",
    "print(f\"Number of CAN IDs: {num_can_ids}\")\n",
    "print(f\"Feature vector length: {feature_vec_length-1}\") # minus one because the can id is the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "lstm_units = 128 # defined in canolo paper\n",
    "dense_units = 256 # defined in canolo paper\n",
    "dropout_rate = 0.2 # defined in canolo paper\n",
    "num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# Model\n",
    "model = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = ambient_loader.batch_size\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    ambient_loader.__getitem__(0)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a forward pass with a batch of data\n",
    "reconstructed_output = model(test_batch_can_ids, test_feature_vec)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "error = mse_loss(reconstructed_output, test_feature_vec)\n",
    "print(\"Reconstruction Error:\", error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_sec = 0.08151132106781006 * ambient_loader.num_batches\n",
    "print(f\"Time in seconds: {time_in_sec}\")\n",
    "time_in_min = time_in_sec / 60\n",
    "print(f\"Time in minutes: {time_in_min}\")\n",
    "time_in_hours = time_in_min / 60\n",
    "print(f\"Time in hours: {time_in_hours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSEUDO_EPOCH_SIZE = 3000\n",
    "\n",
    "def validate_model(model, validation_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches_to_validate = 1000\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for i, batch in enumerate(validation_loader):\n",
    "            can_ids, features = batch\n",
    "            \n",
    "            if i == num_batches_to_validate:\n",
    "                break\n",
    "            \n",
    "            # Forward pass: compute the model output\n",
    "            reconstructed = model(can_ids, features)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    model.train()  # Revert to training mode\n",
    "    num_processed_batches = validation_loader.batch_size * num_batches_to_validate\n",
    "    avg_loss = total_loss / num_processed_batches\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, loss_fn, optimizer, num_epochs, validation_interval):\n",
    "    total_train_loss = 0\n",
    "    pseudo_epoch = 1\n",
    "    num_processed_batches_in_epoch = train_loader.batch_size * PSEUDO_EPOCH_SIZE\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        can_ids, features = batch\n",
    "        print(f\"{i}\", end=\"\\r\")\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        reconstructed = model(can_ids, features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if i % PSEUDO_EPOCH_SIZE == 0:\n",
    "\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Validate model\n",
    "            validation_loss = validate_model(model, validation_loader, loss_fn)\n",
    "            print(f\"Psuedo Epoch {pseudo_epoch}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "            # Show training progress\n",
    "            avg_train_loss = total_train_loss / num_processed_batches_in_epoch\n",
    "            print(f\"Epoch {pseudo_epoch-1}, Average Training Loss: {avg_train_loss}\")\n",
    "            \n",
    "            if pseudo_epoch > num_epochs:\n",
    "                break\n",
    "            \n",
    "\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), f'./saved_model/canolo_model_{pseudo_epoch}.pt')\n",
    "\n",
    "            # save metadata\n",
    "            total_batches_processed = i\n",
    "\n",
    "            metadata = {\n",
    "                \"total_batches_processed\": total_batches_processed,\n",
    "                \"total_train_loss\": total_train_loss,\n",
    "                \"avg_train_loss\": avg_train_loss,\n",
    "                \"validation_loss\": validation_loss\n",
    "            }\n",
    "\n",
    "            with open(f'training_metadata.tsv', 'a') as f:\n",
    "                f.write('\\t'.join(str(metadata[key]) for key in metadata.keys()) + '\\n')\n",
    "\n",
    "            pseudo_epoch += 1\n",
    "            total_train_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "            \n",
    "train_model(model, ambient_loader, validation_loader, loss_fn, optimizer, num_epochs, validation_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CANnoloAutoencoder(\n",
       "  (embedding): Embedding(1789, 105)\n",
       "  (encoder_dense): Linear(in_features=150, out_features=256, bias=True)\n",
       "  (encoder_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (encoder_lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_dense): Linear(in_features=128, out_features=45, bias=True)\n",
       "  (decoder_output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 1: Initialize the model\n",
    "# # Hyperparameters\n",
    "# embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "# lstm_units = 128 # defined in canolo paper\n",
    "# dense_units = 256 # defined in canolo paper\n",
    "# dropout_rate = 0.2 # defined in canolo paper\n",
    "# num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# # Model\n",
    "# model2 = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# # Step 2: Load the state dictionary\n",
    "# state_dict = torch.load(\"./saved_model/canolo_model_1.pt\")\n",
    "# model2.load_state_dict(state_dict)\n",
    "\n",
    "# # If you want to use the model for inference, switch to evaluation mode\n",
    "# model2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
