{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CANnoloAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings):\n",
    "        super(CANnoloAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.encoder_dense = nn.Linear(embedding_dim+45, dense_units)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_lstm = nn.LSTM(input_size=dense_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(input_size=lstm_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(lstm_units, 45)\n",
    "        self.decoder_output = nn.Sigmoid()  # To reconstruct the original packets\n",
    "\n",
    "    def forward(self, can_ids, features):\n",
    "        # Encoding\n",
    "        embedded_ids = self.embedding(can_ids)\n",
    "        # You might need to concatenate the embedded IDs with other features\n",
    "        x = torch.cat([embedded_ids, features], dim=1)\n",
    "        x = torch.tanh(self.encoder_dense(x))\n",
    "        x = self.encoder_dropout(x)\n",
    "        x, _ = self.encoder_lstm(x)\n",
    "\n",
    "        # Decoding\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        x = self.decoder_dense(x)\n",
    "        reconstructed = self.decoder_output(x)\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "        # return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ambient and attack directories.\n",
      "Loading CAN metadata...\n",
      "Parquet files found...\n",
      "Found processed parquet files...\n",
      "Loading processed parquet files...\n",
      "Loading processing data into 'CanData' structure\n"
     ]
    }
   ],
   "source": [
    "from CanDataLoader import CanDataLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "dataset = CanDataLoader(data_path, log_verbosity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"delta_time_last_msg\": {\n",
    "        \"specific_to_can_id\": False,\n",
    "        \"records_back\": 30\n",
    "    },\n",
    "    \"delta_time_last_same_aid\": {\n",
    "        \"specific_to_can_id\": True,\n",
    "        \"records_back\": 15\n",
    "    },\n",
    "}\n",
    "\n",
    "ambient_loader, validation_loader, attack_loader = dataset.prepare_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_can_ids = len(dataset.get_unique_can_ids())\n",
    "window_size = ambient_loader.features_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(num_can_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = num_can_ids  # Example value\n",
    "lstm_units = 128\n",
    "dense_units = 256\n",
    "dropout_rate = 0.2\n",
    "num_embeddings = max(dataset.get_unique_can_ids()) + 1 \n",
    "\n",
    "# Model\n",
    "model = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    ambient_loader.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_can_ids, test_batch_features = next(iter(ambient_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(263)\n",
      "tensor([9.5367e-07, 1.1921e-06, 1.0140e-03, 9.5367e-07, 9.5367e-07, 9.5367e-07,\n",
      "        9.9611e-04, 9.5367e-07, 9.5367e-07, 9.5367e-07, 1.0190e-03, 1.0362e-03,\n",
      "        1.9073e-06, 9.9897e-04, 1.2989e-03, 1.0111e-03, 1.2510e-03, 9.9897e-04,\n",
      "        9.5367e-07, 9.5367e-07, 1.1921e-06, 1.0159e-03, 2.2759e-03, 1.0021e-03,\n",
      "        9.5367e-07, 9.5367e-07, 1.1921e-06, 1.0359e-03, 1.5974e-05, 9.5367e-07,\n",
      "        2.0255e-02, 1.9710e-02, 1.9943e-02, 2.0122e-02, 1.9951e-02, 2.0268e-02,\n",
      "        1.9752e-02, 1.9815e-02, 2.0156e-02, 2.0015e-02, 2.0244e-02, 1.9769e-02,\n",
      "        1.9776e-02, 2.0198e-02, 2.0007e-02])\n"
     ]
    }
   ],
   "source": [
    "print(test_batch_can_ids[0])\n",
    "print(test_batch_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error: 0.2311510145664215\n"
     ]
    }
   ],
   "source": [
    "# Running a forward pass with a batch of data\n",
    "test_batch_can_ids, test_batch_features = next(iter(ambient_loader))\n",
    "reconstructed_output = model(test_batch_can_ids, test_batch_features)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "error = mse_loss(reconstructed_output, test_batch_features)\n",
    "print(\"Reconstruction Error:\", error.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validation_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        # for can_ids, features in validation_loader:\n",
    "        #     output = model(can_ids, features)\n",
    "        #     loss = loss_fn(output, features)  # Adjust depending on your exact setup\n",
    "        #     total_loss += loss.item()\n",
    "\n",
    "        for i, (can_ids, features) in enumerate(validation_loader):\n",
    "            if i % 1000 == 0:\n",
    "                break\n",
    "            print(f\"{i}\\r\")\n",
    "            # Forward pass: compute the model output\n",
    "            reconstructed = model(can_ids, features)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(validation_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 1.1226476892860105e-07\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "2440\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "train_loader = ambient_loader\n",
    "validation_interval = 1000 \n",
    "\n",
    "\n",
    "model.train()  # Set the model to training mode\n",
    "total_train_loss = 0\n",
    "pseudo_epoch_size = 5000\n",
    "pseudo_epoch = 1\n",
    "\n",
    "for i, (can_ids, features) in enumerate(train_loader):\n",
    "    print(f\"{i}\", end=\"\\r\")\n",
    "\n",
    "    # Forward pass: compute the model output\n",
    "    reconstructed = model(can_ids, features)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "    total_train_loss += loss.item()\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear existing gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if i % pseudo_epoch_size == 0:\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {pseudo_epoch}, Average Training Loss: {avg_train_loss}\")\n",
    "        pseudo_epoch += 1\n",
    "        if pseudo_epoch > num_epochs:\n",
    "            break\n",
    "\n",
    "    if i % validation_interval == 0:\n",
    "        # Validate the model at specified intervals\n",
    "        validation_loss = validate_model(model, validation_loader, loss_fn)\n",
    "        print(f\"Psuedo Epoch {pseudo_epoch}, Validation Loss: {validation_loss}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
