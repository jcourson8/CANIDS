{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ambient and attack directories.\n",
      "Loading CAN metadata...\n",
      "Parquet files found...\n",
      "Found processed parquet files...\n",
      "Loading processed parquet files...\n",
      "Loading processing data into 'CanData' structure\n"
     ]
    }
   ],
   "source": [
    "from CanDataset import CanDataset\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "dataset = CanDataset(data_path, log_verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANnoloAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings):\n",
    "        super(CANnoloAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.encoder_dense = nn.Linear(embedding_dim+45, dense_units)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_lstm = nn.LSTM(input_size=dense_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(input_size=lstm_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(lstm_units, 45)\n",
    "        self.decoder_output = nn.Sigmoid()  # To reconstruct the original packets\n",
    "\n",
    "    def forward(self, can_ids, features):\n",
    "        # Encoding\n",
    "        embedded_ids = self.embedding(can_ids)\n",
    "        # You might need to concatenate the embedded IDs with other features\n",
    "        x = torch.cat([embedded_ids, features], dim=1)\n",
    "        x = torch.tanh(self.encoder_dense(x))\n",
    "        x = self.encoder_dropout(x)\n",
    "        x, _ = self.encoder_lstm(x)\n",
    "\n",
    "        # Decoding\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        x = self.decoder_dense(x)\n",
    "        reconstructed = self.decoder_output(x)\n",
    "\n",
    "        return reconstructed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANID   -\n",
    "# f1      -        -  reconstructed_f1\n",
    "# f2      -   -    -  reconstructed_f2\n",
    "# f3      -        -  reconstructed_f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>aid</th>\n",
       "      <th>data</th>\n",
       "      <th>delta_time_last_msg</th>\n",
       "      <th>delta_time_last_same_aid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>852</td>\n",
       "      <td>1FFF40000003C580</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>1505</td>\n",
       "      <td>893FE0070A000080</td>\n",
       "      <td>1.192093e-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>651</td>\n",
       "      <td>0000000000000000</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000992</td>\n",
       "      <td>167</td>\n",
       "      <td>005108E5112A00A0</td>\n",
       "      <td>9.899139e-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000994</td>\n",
       "      <td>722</td>\n",
       "      <td>0000500000000000</td>\n",
       "      <td>2.145767e-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192012</th>\n",
       "      <td>86.459022</td>\n",
       "      <td>560</td>\n",
       "      <td>F700000A7C000E00</td>\n",
       "      <td>9.698868e-04</td>\n",
       "      <td>0.019958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192013</th>\n",
       "      <td>86.461950</td>\n",
       "      <td>339</td>\n",
       "      <td>00000000000C1002</td>\n",
       "      <td>2.928019e-03</td>\n",
       "      <td>0.019914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192014</th>\n",
       "      <td>86.462905</td>\n",
       "      <td>1634</td>\n",
       "      <td>4E60000040000000</td>\n",
       "      <td>9.551048e-04</td>\n",
       "      <td>0.019882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192015</th>\n",
       "      <td>86.462906</td>\n",
       "      <td>412</td>\n",
       "      <td>02FC200002002730</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.019881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192016</th>\n",
       "      <td>86.462907</td>\n",
       "      <td>14</td>\n",
       "      <td>2054560208087530</td>\n",
       "      <td>9.536743e-07</td>\n",
       "      <td>0.009924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192017 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time   aid              data  delta_time_last_msg  \\\n",
       "0        0.000000   852  1FFF40000003C580         0.000000e+00   \n",
       "1        0.000001  1505  893FE0070A000080         1.192093e-06   \n",
       "2        0.000002   651  0000000000000000         9.536743e-07   \n",
       "3        0.000992   167  005108E5112A00A0         9.899139e-04   \n",
       "4        0.000994   722  0000500000000000         2.145767e-06   \n",
       "...           ...   ...               ...                  ...   \n",
       "192012  86.459022   560  F700000A7C000E00         9.698868e-04   \n",
       "192013  86.461950   339  00000000000C1002         2.928019e-03   \n",
       "192014  86.462905  1634  4E60000040000000         9.551048e-04   \n",
       "192015  86.462906   412  02FC200002002730         9.536743e-07   \n",
       "192016  86.462907    14  2054560208087530         9.536743e-07   \n",
       "\n",
       "        delta_time_last_same_aid  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "192012                  0.019958  \n",
       "192013                  0.019914  \n",
       "192014                  0.019882  \n",
       "192015                  0.019881  \n",
       "192016                  0.009924  \n",
       "\n",
       "[192017 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.attack_data.accelerator_attack_drive_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define config\n",
    "This is what we feed to the CanDataset object to create a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"delta_time_last_msg\": {\n",
    "        \"specific_to_can_id\": False,\n",
    "        \"records_back\": 30\n",
    "    },\n",
    "    \"delta_time_last_same_aid\": {\n",
    "        \"specific_to_can_id\": True,\n",
    "        \"records_back\": 15\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `get_dataloaders` on CanDataset object to get the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_loader, validation_loader, attack_loader = dataset.get_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "From the config we defined:\n",
    "    - Batch size of `32`\n",
    "    - Keep track of the current Can ID.\n",
    "    - want the last `30` `delta_time_last_msg`\n",
    "    - want the last `15` `delta_time_last_same_aid`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 204,  253,   60,  519, 1225,  470,  420,  263,  263,  186,  485,  813,\n",
       "          953,  541,  263,  241,  263,  186,   65,  263,  663, 1455,  813,   60,\n",
       "          519, 1225,  470, 1049,  263,  186,  263,  813]),\n",
       " tensor([[0.0000e+00, 9.5367e-07, 9.0849e-03,  ..., 9.9996e-01, 9.9994e-01,\n",
       "          1.0002e+00],\n",
       "         [9.5367e-07, 9.0849e-03, 9.9890e-03,  ..., 1.0000e+00, 1.0001e+00,\n",
       "          9.9888e-01],\n",
       "         [9.0849e-03, 9.9890e-03, 2.0004e-02,  ..., 9.9979e-02, 1.0000e-01,\n",
       "          1.0117e-01],\n",
       "         ...,\n",
       "         [9.5367e-07, 9.5367e-07, 4.3540e-03,  ..., 3.9964e-02, 3.9967e-02,\n",
       "          4.0054e-02],\n",
       "         [9.5367e-07, 4.3540e-03, 1.0190e-03,  ..., 1.9973e-02, 1.9998e-02,\n",
       "          2.0013e-02],\n",
       "         [4.3540e-03, 1.0190e-03, 3.0994e-06,  ..., 5.0041e-02, 5.0938e-02,\n",
       "          4.7960e-02]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_data = ambient_loader.__getitem__(0) # input normally acts as index, but this does not really work as an index. More like get next item.\n",
    "display(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Represents Can ID: \n",
      "204\n",
      "\n",
      "Represents Feature Vector: \n",
      "tensor([0.0000e+00, 9.5367e-07, 9.0849e-03, 9.9890e-03, 2.0004e-02, 9.7489e-04,\n",
      "        1.9014e-02, 9.9111e-04, 6.4800e-03, 1.0200e-03, 9.5367e-07, 9.5367e-07,\n",
      "        1.2046e-02, 1.0159e-03, 1.8436e-02, 1.0025e-02, 1.0021e-02, 9.9802e-04,\n",
      "        1.0180e-03, 1.7963e-02, 1.7386e-02, 2.6519e-03, 1.0040e-03, 1.9073e-06,\n",
      "        1.1921e-06, 1.0140e-03, 1.9073e-06, 9.5367e-07, 1.0171e-03, 9.5367e-07,\n",
      "        1.0000e+00, 1.0000e+00, 1.0001e+00, 9.9998e-01, 1.0000e+00, 9.9993e-01,\n",
      "        1.0001e+00, 1.0001e+00, 9.9992e-01, 1.0000e+00, 9.9998e-01, 1.0021e+00,\n",
      "        9.9996e-01, 9.9994e-01, 1.0002e+00])\n"
     ]
    }
   ],
   "source": [
    "test_batch_can_ids, test_feature_vec = example_data\n",
    "\n",
    "print(f'Represents Can ID: \\n{test_batch_can_ids[0]}\\n')\n",
    "print(f'Represents Feature Vector: \\n{test_feature_vec[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `example_data` is a tuple containing a list of 32 (batch_size) Can ID's and the feature vectors defined in the config.\n",
    "\n",
    "([`tensor containing Can ID's`],[`tensor containing features`])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CAN IDs: 105\n",
      "Feature vector length: 45\n"
     ]
    }
   ],
   "source": [
    "unique_can_ids = dataset.get_unique_can_ids()\n",
    "num_can_ids = len(unique_can_ids)\n",
    "feature_vec_length = ambient_loader.features_len\n",
    "print(f\"Number of CAN IDs: {num_can_ids}\")\n",
    "print(f\"Feature vector length: {feature_vec_length-1}\") # minus one because the can id is the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "lstm_units = 128 # defined in canolo paper\n",
    "dense_units = 256 # defined in canolo paper\n",
    "dropout_rate = 0.2 # defined in canolo paper\n",
    "num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# Model\n",
    "model = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings).to('mps')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = ambient_loader.batch_size\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.544073104858398\n",
      "Time per batch: 0.06544073104858399\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    ambient_loader.__getitem__(0)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Running a forward pass with a batch of data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reconstructed_output \u001b[39m=\u001b[39m model(test_batch_can_ids, test_feature_vec)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mse_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m error \u001b[39m=\u001b[39m mse_loss(reconstructed_output, test_feature_vec)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, can_ids, features):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     embedded_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(can_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# You might need to concatenate the embedded IDs with other features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamescourson/Documents/CANIDS/code/data_prep/torchmodel.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([embedded_ids, features], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Running a forward pass with a batch of data\n",
    "reconstructed_output = model(test_batch_can_ids, test_feature_vec)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "error = mse_loss(reconstructed_output, test_feature_vec)\n",
    "print(\"Reconstruction Error:\", error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_sec = 0.08151132106781006 * ambient_loader.num_batches\n",
    "print(f\"Time in seconds: {time_in_sec}\")\n",
    "time_in_min = time_in_sec / 60\n",
    "print(f\"Time in minutes: {time_in_min}\")\n",
    "time_in_hours = time_in_min / 60\n",
    "print(f\"Time in hours: {time_in_hours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\r"
     ]
    }
   ],
   "source": [
    "PSEUDO_EPOCH_SIZE = 3000\n",
    "\n",
    "def validate_model(model, validation_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches_to_validate = 1000\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for i, batch in enumerate(validation_loader):\n",
    "            can_ids, features = batch\n",
    "            can_ids, features = can_ids.to('mps'), features.to('mps')\n",
    "            \n",
    "            if i == num_batches_to_validate:\n",
    "                break\n",
    "            \n",
    "            # Forward pass: compute the model output\n",
    "            reconstructed = model(can_ids, features)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    model.train()  # Revert to training mode\n",
    "    num_processed_batches = validation_loader.batch_size * num_batches_to_validate\n",
    "    avg_loss = total_loss / num_processed_batches\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, loss_fn, optimizer, num_epochs):\n",
    "    total_train_loss = 0\n",
    "    pseudo_epoch = 1\n",
    "    num_processed_batches_in_epoch = train_loader.batch_size * PSEUDO_EPOCH_SIZE\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        can_ids, features = batch\n",
    "        can_ids, features = can_ids.to('mps'), features.to('mps')\n",
    "        print(f\"{i}\", end=\"\\r\")\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        reconstructed = model(can_ids, features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if i % PSEUDO_EPOCH_SIZE == 0:\n",
    "\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Validate model\n",
    "            validation_loss = validate_model(model, validation_loader, loss_fn)\n",
    "            print(f\"Psuedo Epoch {pseudo_epoch}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "            # Show training progress\n",
    "            avg_train_loss = total_train_loss / num_processed_batches_in_epoch\n",
    "            print(f\"Epoch {pseudo_epoch-1}, Average Training Loss: {avg_train_loss}\")\n",
    "            \n",
    "            if pseudo_epoch > num_epochs:\n",
    "                break\n",
    "            \n",
    "\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), f'./saved_model/canolo_model_{pseudo_epoch}.pt')\n",
    "\n",
    "            # save metadata\n",
    "            total_batches_processed = i\n",
    "\n",
    "            metadata = {\n",
    "                \"total_batches_processed\": total_batches_processed,\n",
    "                \"total_train_loss\": total_train_loss,\n",
    "                \"avg_train_loss\": avg_train_loss,\n",
    "                \"validation_loss\": validation_loss\n",
    "            }\n",
    "\n",
    "            with open(f'training_metadata.tsv', 'a') as f:\n",
    "                f.write('\\t'.join(str(metadata[key]) for key in metadata.keys()) + '\\n')\n",
    "\n",
    "            pseudo_epoch += 1\n",
    "            total_train_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "train_model(model, ambient_loader, validation_loader, loss_fn, optimizer, num_epochs)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.8549"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(ambient_loader)/3000) * 6) /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CANnoloAutoencoder(\n",
       "  (embedding): Embedding(1789, 105)\n",
       "  (encoder_dense): Linear(in_features=150, out_features=256, bias=True)\n",
       "  (encoder_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (encoder_lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
       "  (decoder_dense): Linear(in_features=128, out_features=45, bias=True)\n",
       "  (decoder_output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 1: Initialize the model\n",
    "# # Hyperparameters\n",
    "# embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "# lstm_units = 128 # defined in canolo paper\n",
    "# dense_units = 256 # defined in canolo paper\n",
    "# dropout_rate = 0.2 # defined in canolo paper\n",
    "# num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# # Model\n",
    "# model2 = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# # Step 2: Load the state dictionary\n",
    "# state_dict = torch.load(\"./saved_model/canolo_model_1.pt\")\n",
    "# model2.load_state_dict(state_dict)\n",
    "\n",
    "# # If you want to use the model for inference, switch to evaluation mode\n",
    "# model2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
