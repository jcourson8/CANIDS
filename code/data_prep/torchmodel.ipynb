{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ambient and attack directories.\n",
      "Loading CAN metadata...\n",
      "Parquet files found...\n",
      "Found processed parquet files...\n",
      "Loading processed parquet files...\n",
      "Loading processing data into 'CanData' structure\n"
     ]
    }
   ],
   "source": [
    "from CanDataset import CanDataset\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "dataset = CanDataset(data_path, log_verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANnoloAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings):\n",
    "        super(CANnoloAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.encoder_dense = nn.Linear(embedding_dim+45, dense_units)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder_lstm = nn.LSTM(input_size=dense_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(input_size=lstm_units, hidden_size=lstm_units, num_layers=2, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(lstm_units, 45)\n",
    "        self.decoder_output = nn.Sigmoid()  # To reconstruct the original packets\n",
    "\n",
    "    def forward(self, can_ids, features):\n",
    "        # Encoding\n",
    "        embedded_ids = self.embedding(can_ids)\n",
    "        # You might need to concatenate the embedded IDs with other features\n",
    "        x = torch.cat([embedded_ids, features], dim=1)\n",
    "        x = torch.tanh(self.encoder_dense(x))\n",
    "        x = self.encoder_dropout(x)\n",
    "        x, _ = self.encoder_lstm(x)\n",
    "\n",
    "        # Decoding\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        x = self.decoder_dense(x)\n",
    "        reconstructed = self.decoder_output(x)\n",
    "\n",
    "        return reconstructed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define config\n",
    "This is what we feed to the CanDataset object to create a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"delta_time_last_msg\": {\n",
    "        \"specific_to_can_id\": False,\n",
    "        \"records_back\": 30\n",
    "    },\n",
    "    \"delta_time_last_same_aid\": {\n",
    "        \"specific_to_can_id\": True,\n",
    "        \"records_back\": 15\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `get_dataloaders` on CanDataset object to get the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_loader, validation_loader, attack_loader = dataset.get_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "From the config we defined:\n",
    "    - Batch size of `32`\n",
    "    - Keep track of the current Can ID.\n",
    "    - want the last `30` `delta_time_last_msg`\n",
    "    - want the last `15` `delta_time_last_same_aid`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 204,  253,   60,  519, 1225,  470,  420,  263,  263,  186,  485,  813,\n",
       "          953,  541,  263,  241,  263,  186,   65,  263,  663, 1455,  813,   60,\n",
       "          519, 1225,  470, 1049,  263,  186,  263,  813]),\n",
       " tensor([[0.0000e+00, 9.5367e-07, 9.0849e-03,  ..., 9.9996e-01, 9.9994e-01,\n",
       "          1.0002e+00],\n",
       "         [9.5367e-07, 9.0849e-03, 9.9890e-03,  ..., 1.0000e+00, 1.0001e+00,\n",
       "          9.9888e-01],\n",
       "         [9.0849e-03, 9.9890e-03, 2.0004e-02,  ..., 9.9979e-02, 1.0000e-01,\n",
       "          1.0117e-01],\n",
       "         ...,\n",
       "         [9.5367e-07, 9.5367e-07, 4.3540e-03,  ..., 3.9964e-02, 3.9967e-02,\n",
       "          4.0054e-02],\n",
       "         [9.5367e-07, 4.3540e-03, 1.0190e-03,  ..., 1.9973e-02, 1.9998e-02,\n",
       "          2.0013e-02],\n",
       "         [4.3540e-03, 1.0190e-03, 3.0994e-06,  ..., 5.0041e-02, 5.0938e-02,\n",
       "          4.7960e-02]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_data = ambient_loader.__getitem__(0) # input normally acts as index, but this does not really work as an index. More like get next item.\n",
    "display(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Represents Can ID: \n",
      "204\n",
      "\n",
      "Represents Feature Vector: \n",
      "tensor([0.0000e+00, 9.5367e-07, 9.0849e-03, 9.9890e-03, 2.0004e-02, 9.7489e-04,\n",
      "        1.9014e-02, 9.9111e-04, 6.4800e-03, 1.0200e-03, 9.5367e-07, 9.5367e-07,\n",
      "        1.2046e-02, 1.0159e-03, 1.8436e-02, 1.0025e-02, 1.0021e-02, 9.9802e-04,\n",
      "        1.0180e-03, 1.7963e-02, 1.7386e-02, 2.6519e-03, 1.0040e-03, 1.9073e-06,\n",
      "        1.1921e-06, 1.0140e-03, 1.9073e-06, 9.5367e-07, 1.0171e-03, 9.5367e-07,\n",
      "        1.0000e+00, 1.0000e+00, 1.0001e+00, 9.9998e-01, 1.0000e+00, 9.9993e-01,\n",
      "        1.0001e+00, 1.0001e+00, 9.9992e-01, 1.0000e+00, 9.9998e-01, 1.0021e+00,\n",
      "        9.9996e-01, 9.9994e-01, 1.0002e+00])\n"
     ]
    }
   ],
   "source": [
    "test_batch_can_ids, test_feature_vec = example_data\n",
    "\n",
    "print(f'Represents Can ID: \\n{test_batch_can_ids[0]}\\n')\n",
    "print(f'Represents Feature Vector: \\n{test_feature_vec[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `example_data` is a tuple containing a list of 32 (batch_size) Can ID's and the feature vectors defined in the config.\n",
    "\n",
    "([`tensor containing Can ID's`],[`tensor containing features`])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CAN IDs: 105\n",
      "Feature vector length: 45\n"
     ]
    }
   ],
   "source": [
    "unique_can_ids = dataset.get_unique_can_ids()\n",
    "num_can_ids = len(unique_can_ids)\n",
    "feature_vec_length = ambient_loader.features_len\n",
    "print(f\"Number of CAN IDs: {num_can_ids}\")\n",
    "print(f\"Feature vector length: {feature_vec_length-1}\") # minus one because the can id is the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = num_can_ids  # embedding dimension should be equal to the number of CAN IDs\n",
    "lstm_units = 128 # defined in canolo paper\n",
    "dense_units = 256 # defined in canolo paper\n",
    "dropout_rate = 0.2 # defined in canolo paper\n",
    "num_embeddings = max(unique_can_ids) + 1 # not sure why + 1 rn but it works\n",
    "\n",
    "# Model\n",
    "model = CANnoloAutoencoder(embedding_dim, lstm_units, dense_units, dropout_rate, num_embeddings)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = ambient_loader.batch_size\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.636068105697632\n",
      "Time per batch: 0.06636068105697632\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# time the training\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    ambient_loader.__getitem__(0)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}\")\n",
    "print(f\"Time per batch: {(end-start)/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error: 0.22565299272537231\n"
     ]
    }
   ],
   "source": [
    "# Running a forward pass with a batch of data\n",
    "reconstructed_output = model(test_batch_can_ids, test_feature_vec)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "error = mse_loss(reconstructed_output, test_feature_vec)\n",
    "print(\"Reconstruction Error:\", error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 1.4127334638818533e-07\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Psuedo Epoch 2, Validation Loss: 0.0\n",
      "Epoch 2, Average Training Loss: 9.152403372752713e-05\n",
      "Psuedo Epoch 3, Validation Loss: 0.0\n",
      "Psuedo Epoch 3, Validation Loss: 0.0\n",
      "Psuedo Epoch 3, Validation Loss: 0.0\n",
      "Psuedo Epoch 3, Validation Loss: 0.0\n",
      "Psuedo Epoch 3, Validation Loss: 0.0\n",
      "Epoch 3, Average Training Loss: 0.00016105915937795432\n"
     ]
    }
   ],
   "source": [
    "PSEUDO_EPOCH_SIZE = 500\n",
    "\n",
    "def validate_model(model, validation_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for i, (can_ids, features) in enumerate(validation_loader):\n",
    "            if i % 1000 == 0:\n",
    "                break\n",
    "            print(f\"{i}\\r\")\n",
    "            \n",
    "            # Forward pass: compute the model output\n",
    "            reconstructed = model(can_ids, features)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "    model.train()  # Revert to training mode\n",
    "    avg_loss = total_loss / len(validation_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, loss_fn, optimizer, num_epochs, validation_interval):\n",
    "    total_train_loss = 0\n",
    "    pseudo_epoch = 1\n",
    "\n",
    "    model.train()\n",
    "    for i, (can_ids, features) in enumerate(train_loader):\n",
    "        print(f\"{i}\", end=\"\\r\")\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        reconstructed = model(can_ids, features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(reconstructed, features)  # Ensure correct target is used\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if i % PSEUDO_EPOCH_SIZE == 0:\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            print(f\"Epoch {pseudo_epoch}, Average Training Loss: {avg_train_loss}\")\n",
    "            if pseudo_epoch > num_epochs:\n",
    "                break\n",
    "            pseudo_epoch += 1\n",
    "\n",
    "        if i % validation_interval == 0:\n",
    "            # Validate the model at specified intervals\n",
    "            validation_loss = validate_model(model, validation_loader, loss_fn)\n",
    "            print(f\"Psuedo Epoch {pseudo_epoch}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "num_epochs = 2\n",
    "validation_interval = 100 \n",
    "            \n",
    "train_model(model, ambient_loader, validation_loader, loss_fn, optimizer, num_epochs, validation_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
